{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from hyperparams import Hyperparams as hp\n",
    "import os\n",
    "import codecs\n",
    "from jamo import h2j, j2hcj\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trascript = hp.transcript_pos\n",
    "lines = codecs.open(trascript, 'r', 'utf-8').readlines()\n",
    "\n",
    "if not (os.path.isdir(hp.mels_dir)):\n",
    "    os.mkdir(hp.mels_dir)\n",
    "    print('{%s} does not exist, created {%s}'.format(hp.mels_dir, hp.mels_dir))\n",
    "    \n",
    "if not (os.path.isdir(hp.mags_dir)):\n",
    "    os.mkdir(hp.mags_dir)\n",
    "    print('{%s} does not exist, created {%s}'.format(hp.mags_dir, hp.mags_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    char2idx = {char: idx for idx, char in enumerate(hp.vocab)}\n",
    "    idx2char = {idx: char for idx, char in enumerate(hp.vocab)}\n",
    "    return char2idx, idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 12853/12853 [00:01<00:00, 11558.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 12853/12853 [00:01<00:00, 10376.30it/s]\n"
     ]
    }
   ],
   "source": [
    "fnames, texts, secs, text_lengths = [], [], [], []\n",
    "\n",
    "for line in tqdm(lines):\n",
    "    line = j2hcj(line)\n",
    "    _, _, text, _ = line.strip().split('|')\n",
    "    text_lengths.append(len(text))\n",
    "\n",
    "maxlen = max(text_lengths)\n",
    "\n",
    "char2idx, idx2char = load_vocab();\n",
    "\n",
    "for line in tqdm(lines):\n",
    "    line = j2hcj(line)\n",
    "    fname, _, text, sec = line.strip().split('|')\n",
    "    \n",
    "    padLen = maxlen - len(text)\n",
    "    \n",
    "    text = text + ' ' * padLen\n",
    "    \n",
    "    encodedText = [char2idx[char] for char in text]\n",
    "    \n",
    "#     encodedText = np.array(encodedText, np.int32).tostring()\n",
    "    \n",
    "    fnames.append(fname); \n",
    "    texts.append(encodedText)\n",
    "    secs.append(float(sec)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_texts = preprocessing.sequence.pad_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = Embedding(len(hp.vocab), \n",
    "                        hp.embed_size, \n",
    "                        input_length = maxlen,\n",
    "                        embeddings_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = Input(shape=(None, ), dtype='int32', name='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text = embed_layer(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prenet_layers = Sequential()\n",
    "prenet_layers.add(Dense(hp.num_prenet_node_1, activation='relu'))\n",
    "prenet_layers.add(Dropout(hp.dropout_rate))\n",
    "prenet_layers.add(Dense(hp.num_prenet_node_2, activation='relu'))\n",
    "prenet_layers.add(Dropout(hp.dropout_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 104, 256)          65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 104, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 104, 128)          32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 104, 128)          0         \n",
      "=================================================================\n",
      "Total params: 98,688\n",
      "Trainable params: 98,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prenet_layers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prenet_result = prenet_layers(embedded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1D_outputs = []\n",
    "for k in range(1, hp.K + 1):\n",
    "    conv1D_outputs.append(Conv1D(filters=hp.num_k_filter, kernel_size=k, padding='same')(prenet_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1D_result = layers.concatenate(conv1D_outputs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pooled_result = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv1D_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_proj_1_result = Conv1D(filters=hp.num_conv1d_proj_filter, \n",
    "                            kernel_size=hp.size_conv1d_proj_filter, \n",
    "                            padding='same')(max_pooled_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_1_result = BatchNormalization(axis=1,  \n",
    "                   epsilon=0.001,\n",
    "                   center=True, \n",
    "                   scale=True)(conv_proj_1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_proj_2_result = Conv1D(filters=hp.num_conv1d_proj_filter, \n",
    "                            kernel_size=hp.size_conv1d_proj_filter, \n",
    "                            padding='same')(norm_1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_2_result = BatchNormalization(axis=1,  \n",
    "                   epsilon=0.001,\n",
    "                   center=True, \n",
    "                   scale=True)(conv_proj_2_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_result = layers.add([prenet_result, norm_2_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://gist.github.com/iskandr/a874e4cf358697037d14a17020304535\n",
    "'''\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Activation, Multiply, Add, Lambda\n",
    "import keras.initializers\n",
    "\n",
    "def highway_layers(value, n_layers, activation=\"tanh\", gate_bias=-3):\n",
    "    dim = K.int_shape(value)[-1]\n",
    "    gate_bias_initializer = keras.initializers.Constant(gate_bias)\n",
    "    for i in range(n_layers):     \n",
    "        gate = Dense(units=dim, bias_initializer=gate_bias_initializer)(value)\n",
    "        gate = Activation(\"sigmoid\")(gate)\n",
    "        negated_gate = Lambda(\n",
    "            lambda x: 1.0 - x,\n",
    "            output_shape=(dim,))(gate)\n",
    "        transformed = Dense(units=dim)(value)\n",
    "        transformed = Activation(activation)(transformed)\n",
    "        transformed_gated = Multiply()([gate, transformed])\n",
    "        identity_gated = Multiply()([negated_gate, value])\n",
    "        value = Add()([transformed_gated, identity_gated])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_result = highway_layers(residual_result, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_15/add:0' shape=(?, 104, 128) dtype=float32>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highway_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, CuDNNGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1/add:0' shape=(?, 104, 128) dtype=float32>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_result = layers.Bidirectional(CuDNNGRU(128), merge_mode='concat')(highway_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_4/concat_2:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 256)\n"
     ]
    }
   ],
   "source": [
    "model = Model(text_input, gru_result)\n",
    "x = model.predict({'text': padded_texts[0:5]})\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def py_func(fname):\n",
    "\n",
    "    npy_name = fname.split('/')[1].replace('wav', 'npy')\n",
    "    mel_path = os.path.join(hp.mels_dir, npy_name)\n",
    "    mag_path = os.path.join(hp.mags_dir, npy_name)\n",
    "    fpath = os.path.join(hp.data_dir, fname)\n",
    "    wav, fs = librosa.core.load(fpath, mono=True)\n",
    "\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-e42ffa56fb8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mname_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\주환\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\주환\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mask'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[0marguments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mask'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-117-384eb489c12e>\u001b[0m in \u001b[0;36mpy_func\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mnpy_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wav'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mmel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmels_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpy_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmag_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmags_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpy_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "name_input = Input(shape=(9, 10))\n",
    "lll = Lambda(py_func)(name_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda_18/Identity:0' shape=(?, 9, 10) dtype=float32>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n",
    "    lookup_table = tf.get_variable('lookup_table', \n",
    "                                   dtype=tf.float32, \n",
    "                                   shape=[len(hp.vocab), hp.embed_size],\n",
    "                                   initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_text = tf.nn.embedding_lookup(lookup_table, text_decoded)\n",
    "embed_text = tf.expand_dims(embed_text, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(embed_text)\n",
    "    plt.imshow(x[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"prenet\", reuse=tf.AUTO_REUSE):\n",
    "    outputs = tf.layers.dense(embed_text, units=hp.num_prenet_node_1, activation=tf.nn.relu, name=\"dense1\")\n",
    "    outputs = tf.layers.dropout(outputs, rate=hp.dropout_rate, name=\"dropout1\")\n",
    "    outputs = tf.layers.dense(outputs, units=hp.num_prenet_node_2, activation=tf.nn.relu, name=\"dense2\")\n",
    "    outputs = tf.layers.dropout(outputs, rate=hp.dropout_rate, name=\"dropout2\") \n",
    "\n",
    "    prenet_result = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(prenet_result)\n",
    "    plt.imshow(x[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"conv1d_banks\", reuse=tf.AUTO_REUSE):\n",
    "    for k in range(1, hp.K + 1):\n",
    "        with tf.variable_scope(\"filter_num_{}\".format(k)):\n",
    "            params = {\"inputs\":prenet_result, \"filters\":hp.num_k_filter, \"kernel_size\":k,\n",
    "                    \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                    \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "\n",
    "            # Works when resue = True\n",
    "            # For i loop, filter is reused.\n",
    "\n",
    "            conv_outputs = tf.layers.conv1d(**params)\n",
    "            if k == 1:\n",
    "                conv_bank_outputs = conv_outputs\n",
    "            else:\n",
    "                conv_bank_outputs = tf.concat((conv_bank_outputs, conv_outputs), axis=2)\n",
    "\n",
    "    conv_bank_result = conv_bank_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(conv_bank_result)\n",
    "    plt.imshow(x[0, :, :], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " max_pooling_result = tf.layers.max_pooling1d(conv_bank_result, pool_size=2, strides=1, padding=\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(max_pooling_result)\n",
    "    plt.imshow(x[0, :, :], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_bank_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"conv1d_1\"):       \n",
    "    params = {\"inputs\":max_pooling_result, \"filters\":hp.num_conv1d_proj_filter, \"kernel_size\":hp.size_conv1d_proj_filter,\n",
    "                    \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                    \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "    conv_proj_1_result = tf.layers.conv1d(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(conv_proj_1_result)\n",
    "    plt.imshow(x[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"conv1d_1\"):\n",
    "    bn_1_result = tf.contrib.layers.batch_norm(inputs=conv_proj_1_result,\n",
    "                                           center=True,\n",
    "                                           scale=True,\n",
    "                                           updates_collections=None,\n",
    "                                           is_training=True,\n",
    "                                           scope=\"conv1d_1\",\n",
    "                                           fused=True,\n",
    "                                           reuse=tf.AUTO_REUSE)\n",
    "    batch_norm_1_result = tf.nn.relu(bn_1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(batch_norm_1_result)\n",
    "    plt.imshow(x[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"conv1d_2\"):\n",
    "    params = {\"inputs\":batch_norm_1_result, \"filters\":hp.num_conv1d_proj_filter, \"kernel_size\":hp.size_conv1d_proj_filter,\n",
    "                    \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                    \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "    conv_proj_2_result = tf.layers.conv1d(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(conv_proj_2_result)\n",
    "    plt.imshow(x[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"conv1d_2\"):\n",
    "    bn_2_result = tf.contrib.layers.batch_norm(inputs=conv_proj_2_result,\n",
    "                                           center=True,\n",
    "                                           scale=True,\n",
    "                                           updates_collections=None,\n",
    "                                           is_training=True,\n",
    "                                           scope=\"conv1d_2\",\n",
    "                                           fused=True,\n",
    "                                           reuse=tf.AUTO_REUSE)\n",
    "    batch_norm_2_result = tf.nn.relu(bn_2_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(batch_norm_2_result)\n",
    "    plt.imshow(x[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_output = prenet_result + batch_norm_2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(res_output)\n",
    "    plt.imshow(x[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highwaynet_output = []\n",
    "\n",
    "for i in range(hp.num_highwaynet_blocks):\n",
    "    scope = \"highwaynet_{:d}\".format(i)\n",
    "    with tf.variable_scope(scope):\n",
    "        \n",
    "        if i == 0:\n",
    "            highwaynet_input = res_output\n",
    "        else:\n",
    "            highwaynet_input = highwaynet_output\n",
    "\n",
    "        H = tf.layers.dense(highwaynet_input, units=hp.num_highwaynet_units, activation=tf.nn.relu, name=\"dense1\", reuse=tf.AUTO_REUSE)\n",
    "        T = tf.layers.dense(highwaynet_input, units=hp.num_highwaynet_units, activation=tf.nn.sigmoid,\n",
    "                            bias_initializer=tf.constant_initializer(-1.0), name=\"dense2\", reuse=tf.AUTO_REUSE)\n",
    "        highwaynet_output = H*T + highwaynet_input*(1.-T)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highwaynet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(highwaynet_output)\n",
    "    plt.imshow(x[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"gru\", reuse=tf.AUTO_REUSE):\n",
    "#     cell = tf.contrib.rnn.GRUCell(hp.num_gru_units)\n",
    "#     cell_bw = tf.contrib.rnn.GRUCell(hp.num_gru_units)\n",
    "    cell = tf.contrib.rnn.GRUCell(128)\n",
    "    cell_bw = tf.contrib.rnn.GRUCell(128)\n",
    "\n",
    "    output, _ = tf.nn.bidirectional_dynamic_rnn(cell, cell_bw, highwaynet_output, dtype=tf.float32)\n",
    "    gru_result = tf.concat(output, 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder_prenet\", reuse=tf.AUTO_REUSE):\n",
    "    outputs = tf.layers.dense(decoder_input, units=hp.num_prenet_node_1, activation=tf.nn.relu, name=\"dense1\")\n",
    "    outputs = tf.layers.dropout(outputs, rate=hp.dropout_rate, name=\"dropout1\")\n",
    "    outputs = tf.layers.dense(outputs, units=hp.num_prenet_node_2, activation=tf.nn.relu, name=\"dense2\")\n",
    "    decoder_prenet_result = tf.layers.dropout(outputs, rate=hp.dropout_rate, name=\"dropout2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_prenet_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(decoder_prenet_result)\n",
    "    plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_prenet_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_prenet_result_4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"attention_decoder\", reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "    decoder_prenet_result_4D = tf.expand_dims(decoder_prenet_result, 0)\n",
    "\n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(hp.num_attention_units, gru_result)\n",
    "    decoder_cell = tf.contrib.rnn.GRUCell(hp.num_gru_units)\n",
    "    cell_with_attention = tf.contrib.seq2seq.AttentionWrapper(decoder_cell,\n",
    "                                                              attention_mechanism,\n",
    "                                                              hp.num_attention_units,\n",
    "                                                              alignment_history=True)\n",
    "    dec, state = tf.nn.dynamic_rnn(cell_with_attention, decoder_prenet_result_4D, dtype=tf.float32)\n",
    "#     dec, state = keras.layers.RNN(cell_with_attention, decoder_prenet_result_4D, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_prenet_result_4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(state)\n",
    "    x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"attention_decoder\", reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "    alignment = tf.transpose(state.alignment_history.stack(),[1,2,0])\n",
    "\n",
    "    with tf.variable_scope(\"decoder_gru_1\", reuse=tf.AUTO_REUSE):\n",
    "        cell = tf.contrib.rnn.GRUCell(128)\n",
    "        output, _ = tf.nn.dynamic_rnn(cell, dec, dtype=tf.float32)\n",
    "        gru_output_1 = tf.concat(output, 2)\n",
    "\n",
    "    dec = dec + gru_output_1\n",
    "\n",
    "    with tf.variable_scope(\"decoder_gru_2\", reuse=tf.AUTO_REUSE):\n",
    "        cell = tf.contrib.rnn.GRUCell(128)\n",
    "        output, _ = tf.nn.dynamic_rnn(cell, dec, dtype=tf.float32)\n",
    "        gru_output_2 = tf.concat(output, 2)\n",
    "\n",
    "    dec = dec + gru_output_2\n",
    "\n",
    "    # Outputs => (N, T_y/r, n_mels*r)\n",
    "    y_hat = tf.layers.dense(dec, hp.n_mels*hp.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(y_hat)\n",
    "    plt.imshow(x[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "    dec_2_input = tf.reshape(y_hat, [1, -1, hp.n_mels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(\"conv1d_banks\", reuse=tf.AUTO_REUSE):\n",
    "        for k in range(1, hp.K + 1):\n",
    "            with tf.variable_scope(\"filter_num_{}\".format(k)):\n",
    "                params = {\"inputs\":dec_2_input, \"filters\":hp.num_k_filter, \"kernel_size\":k,\n",
    "                        \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                        \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "\n",
    "                # Works when resue = True\n",
    "                # For i loop, filter is reused.\n",
    "\n",
    "                conv_outputs = tf.layers.conv1d(**params)\n",
    "                if k == 1:\n",
    "                    conv_bank_outputs = conv_outputs\n",
    "                else:\n",
    "                    conv_bank_outputs = tf.concat((conv_bank_outputs, conv_outputs), axis=2)\n",
    "\n",
    "    dec_2_conv_bank_result = conv_bank_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "    dec_2_max_pooling_result = tf.layers.max_pooling1d(dec_2_conv_bank_result, pool_size=2, strides=1, padding=\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(\"conv1d_1\"):\n",
    "        params = {\"inputs\":dec_2_max_pooling_result, \"filters\":hp.num_conv1d_proj_filter, \"kernel_size\":hp.size_conv1d_proj_filter,\n",
    "                        \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                        \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "\n",
    "        dec_2_conv_proj_1_result = tf.layers.conv1d(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(\"conv1d_1\"):\n",
    "        bn_1_result = tf.contrib.layers.batch_norm(inputs=dec_2_conv_proj_1_result,\n",
    "                                               center=True,\n",
    "                                               scale=True,\n",
    "                                               updates_collections=None,\n",
    "                                               is_training=True,\n",
    "                                               scope=\"conv1d_1\",\n",
    "                                               fused=True,\n",
    "                                               reuse=tf.AUTO_REUSE)\n",
    "        dec_2_batch_norm_1_result = tf.nn.relu(bn_1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(\"conv1d_2\"):\n",
    "        params = {\"inputs\":dec_2_batch_norm_1_result, \"filters\":hp.num_conv1d_proj_filter, \"kernel_size\":hp.size_conv1d_proj_filter,\n",
    "                        \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                        \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "\n",
    "        dec_2_conv_proj_2_result = tf.layers.conv1d(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(\"conv1d_2\"):\n",
    "        bn_2_result = tf.contrib.layers.batch_norm(inputs=dec_2_conv_proj_2_result,\n",
    "                                               center=True,\n",
    "                                               scale=True,\n",
    "                                               updates_collections=None,\n",
    "                                               is_training=True,\n",
    "                                               scope=\"conv1d_2\",\n",
    "                                               fused=True,\n",
    "                                               reuse=tf.AUTO_REUSE)\n",
    "        dec_2_batch_norm_2_result = tf.nn.relu(bn_2_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "    dec_2_sync_result = tf.layers.dense(dec_2_batch_norm_2_result, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "    for i in range(hp.num_highwaynet_blocks):\n",
    "        scope = \"highwaynet_{:d}\".format(i)\n",
    "        with tf.variable_scope(scope):\n",
    "\n",
    "            if i == 0:\n",
    "                highwaynet_input = dec_2_sync_result\n",
    "            else:\n",
    "                highwaynet_input = highwaynet_output\n",
    "\n",
    "            highwaynet_output = []\n",
    "\n",
    "            H = tf.layers.dense(highwaynet_input, units=hp.num_highwaynet_units, activation=tf.nn.relu, name=\"dense1\", reuse=tf.AUTO_REUSE)\n",
    "            T = tf.layers.dense(highwaynet_input, units=hp.num_highwaynet_units, activation=tf.nn.sigmoid,\n",
    "                                bias_initializer=tf.constant_initializer(-1.0), name=\"dense2\", reuse=tf.AUTO_REUSE)\n",
    "            highwaynet_output = H*T + highwaynet_input*(1.-T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"dec_2_gru\", reuse=tf.AUTO_REUSE):\n",
    "    cell = tf.contrib.rnn.GRUCell(128)\n",
    "    cell_bw = tf.contrib.rnn.GRUCell(128)\n",
    "\n",
    "    output, _ = tf.nn.bidirectional_dynamic_rnn(cell, cell_bw, highwaynet_output, dtype=tf.float32)\n",
    "    dec_2_gru_result = tf.concat(output, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(\"final\", reuse=tf.AUTO_REUSE):\n",
    "        z_hat = tf.layers.dense(dec_2_gru_result, 1 + hp.nsc_sec*hp.fs//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = tf.reduce_mean(tf.abs(y_hat - y))\n",
    "loss2 = tf.reduce_mean(tf.abs(z_hat - mag))\n",
    "loss = loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = sess.run(loss)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "lr = learning_rate_decay(hp.lr, global_step=global_step)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr)\n",
    "\n",
    "## gradient clipping\n",
    "gvs = optimizer.compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped = []\n",
    "\n",
    "for grad, var in gvs:\n",
    "    if grad is None:\n",
    "        clipped.append((grad, var))\n",
    "    else :\n",
    "        grad = tf.clip_by_norm(grad, 5.)\n",
    "        clipped.append((grad, var))\n",
    "    \n",
    "train_op = optimizer.apply_gradients(clipped, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'\n",
    "\n",
    "tf.summary.scalar('{}/loss1'.format(mode), loss1)\n",
    "tf.summary.scalar('{}/loss'.format(mode), loss)\n",
    "tf.summary.scalar('{}/lr'.format(mode), hp.lr)\n",
    "\n",
    "tf.summary.image(\"{}/mel_gt\".format(mode), tf.expand_dims(y, -1), max_outputs=1)\n",
    "tf.summary.image(\"{}/mel_hat\".format(mode), tf.expand_dims(y_hat, -1), max_outputs=1)\n",
    "tf.summary.image(\"{}/mag_gt\".format(mode), tf.expand_dims(mag, -1), max_outputs=1)\n",
    "tf.summary.image(\"{}/mag_hat\".format(mode), tf.expand_dims(z_hat, -1), max_outputs=1)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    for i in range(100):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        _, gs = sess.run([train_op, global_step])\n",
    "        print(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.logdir = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.train.MonitoredTrainingSession(summary_dir=hp.logdir, save_summaries_secs=60) as sess:\n",
    "    while 1:\n",
    "        for _ in tqdm(range(100), total=100, ncols=70, leave=False, unit='b'):\n",
    "            #_, gs = sess.run([train_op, global_step])\n",
    "            \n",
    "            _, gs = sess.run([dec_2_sync_result])\n",
    "\n",
    "            # Write checkpoint files\n",
    "            if gs % 1000 == 0:\n",
    "                sv.saver.save(sess, hp.logdir + '/model_gs_{}k'.format(gs//1000))\n",
    "\n",
    "                # plot the first alignment for logging\n",
    "                al = sess.run(alignment)\n",
    "                plt.imshow(al[0])\n",
    "                ply.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
