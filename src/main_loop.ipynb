{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from hyperparams import Hyperparams as hp\n",
    "import os\n",
    "import codecs\n",
    "from jamo import h2j, j2hcj\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    char2idx = {char: idx for idx, char in enumerate(hp.vocab)}\n",
    "    idx2char = {idx: char for idx, char in enumerate(hp.vocab)}\n",
    "    return char2idx, idx2char\n",
    "\n",
    "def spectrogram(wav, nsc, nov, fs):\n",
    "    \n",
    "    \n",
    "    S = librosa.feature.melspectrogram(y=wav, sr=fs, n_fft=nsc, hop_length=nov, power=2.0)\n",
    "    dbS = 20 * np.log10(np.maximum(S, hp.eps))\n",
    "    \n",
    "    \n",
    "    return dbS\n",
    "\n",
    "def mel_spectrogram(wav, nsc, nov, fs):\n",
    "    \n",
    "    \n",
    "    S = librosa.feature.melspectrogram(y=wav, sr=fs, n_fft=nsc, hop_length=nov, power=2.0, n_mels = hp.n_mels)\n",
    "    dbS = 20 * np.log10(np.maximum(S, hp.eps))\n",
    "    \n",
    "    \n",
    "    return dbS\n",
    "\n",
    "def true_spectrogram(wav, nsc, nov):\n",
    "    \n",
    "    \n",
    "    S = librosa.core.stft(wav, n_fft=nsc, hop_length=nov)\n",
    "    Sxx = abs(S)\n",
    "    dbS = 20 * np.log10(np.maximum(Sxx, hp.eps))\n",
    "    \n",
    "    \n",
    "    return dbS\n",
    "\n",
    "def learning_rate_decay(init_lr, global_step, warmup_steps=4000.):\n",
    "    '''Noam scheme from tensor2tensor'''\n",
    "    step = tf.cast(global_step + 1, dtype=tf.float32)\n",
    "    return init_lr * warmup_steps ** 0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trascript = hp.transcript_pos\n",
    "lines = codecs.open(trascript, 'r', 'utf-8').readlines()\n",
    "\n",
    "if not (os.path.isdir(hp.mels_dir)):\n",
    "    os.mkdir(hp.mels_dir)\n",
    "    print('{%s} does not exist, created {%s}'.format(hp.mels_dir, hp.mels_dir))\n",
    "    \n",
    "if not (os.path.isdir(hp.mags_dir)):\n",
    "    os.mkdir(hp.mags_dir)\n",
    "    print('{%s} does not exist, created {%s}'.format(hp.mags_dir, hp.mags_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 12853/12853 [00:01<00:00, 10009.40it/s]\n"
     ]
    }
   ],
   "source": [
    "fnames, texts, secs, text_lengths = [], [], [], []\n",
    "char2idx, idx2char = load_vocab();\n",
    "\n",
    "for line in tqdm(lines):\n",
    "    line = j2hcj(line)\n",
    "    fname, _, text, sec = line.strip().split('|')\n",
    "    encodedText = [char2idx[char] for char in text]\n",
    "    encodedText = np.array(encodedText, np.int32)#.tostring()\n",
    "    fnames.append(fname); texts.append(encodedText)\n",
    "    secs.append(float(sec)); text_lengths.append(len(encodedText))\n",
    "    \n",
    "fnames_ = np.asarray(fnames)\n",
    "texts_ = np.asarray(texts)\n",
    "secs_ = np.asarray(secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]\n",
      "  0%|                                                                             | 1/1000 [02:30<41:44:01, 150.39s/it]\n",
      "  0%|▏                                                                            | 2/1000 [05:10<42:27:43, 153.17s/it]\n",
      "  0%|▏                                                                            | 3/1000 [07:52<43:11:39, 155.97s/it]\n",
      "  0%|▎                                                                            | 4/1000 [11:04<46:08:52, 166.80s/it]\n",
      "  0%|▍                                                                            | 5/1000 [14:46<50:38:01, 183.20s/it]\n",
      "  1%|▍                                                                            | 6/1000 [18:55<56:06:15, 203.19s/it]\n",
      "  1%|▌                                                                            | 7/1000 [23:32<62:07:57, 225.25s/it]\n",
      "  1%|▌                                                                            | 8/1000 [28:56<70:11:45, 254.74s/it]\n",
      "  1%|▋                                                                            | 9/1000 [34:57<78:54:49, 286.67s/it]\n",
      "  1%|▊                                                                           | 10/1000 [42:09<90:51:28, 330.39s/it]\n",
      "  1%|▊                                                                          | 11/1000 [50:29<104:41:46, 381.10s/it]\n",
      "  1%|▉                                                                          | 12/1000 [59:29<117:43:34, 428.96s/it]"
     ]
    }
   ],
   "source": [
    "#loop_begins\n",
    "\n",
    "for ii in tqdm(range(1000)):\n",
    "\n",
    "    #Random Selection\n",
    "    randIdx = np.random.choice(range(len(lines)), 20)\n",
    "    randIdx.sort()\n",
    "#     print(randIdx)\n",
    "\n",
    "    fnames = fnames_[randIdx]\n",
    "    texts = texts_[randIdx]\n",
    "    secs = secs_[randIdx]\n",
    "\n",
    "    mels, mags, ys = [], [], []\n",
    "\n",
    "    for i in range(0, len(fnames)):\n",
    "        fname = fnames[i]\n",
    "        text = texts[i]\n",
    "\n",
    "        npy_name = fname.split('/')[1].replace('wav', 'npy')\n",
    "        mel_path = os.path.join(hp.mels_dir, npy_name)\n",
    "        mag_path = os.path.join(hp.mags_dir, npy_name)\n",
    "\n",
    "    #     if os.path.isfile(mel_path) and os.path.isfile(mag_path):\n",
    "        if False:\n",
    "\n",
    "            mag = np.load(mag_path)\n",
    "            mel = np.load(mel_path) \n",
    "\n",
    "        else :\n",
    "\n",
    "            fpath = os.path.join(hp.data_dir, fname)\n",
    "            wav, fs = librosa.core.load(fpath, mono=True)\n",
    "            nsc = np.int(fs * hp.nsc_sec)\n",
    "            nov = np.int(fs * hp.nov_sec)\n",
    "            mag_coef = np.mean(spectrogram(wav, nsc, nov, fs), axis=0)\n",
    "            mel = mel_spectrogram(wav, nsc, nov, fs)\n",
    "            mag = true_spectrogram(wav, nsc, nov)\n",
    "\n",
    "            active = np.where(mag_coef > hp.db_limit)[0]\n",
    "\n",
    "            first = active[0]\n",
    "            last = active[-1] + 1\n",
    "\n",
    "            if first - hp.offset >= 0:\n",
    "                first = first - hp.offset\n",
    "            else:\n",
    "                    first = 0\n",
    "\n",
    "            if last + hp.offset < len(mag_coef):\n",
    "                last = last + hp.offset\n",
    "            else:\n",
    "                last = len(mag_coef)\n",
    "\n",
    "            mag = mag[:, first:last]\n",
    "            mel = mel[:, first:last]\n",
    "\n",
    "            mag = mag / hp.max_db\n",
    "            mel = mel / hp.max_db\n",
    "\n",
    "            # Do I really need \n",
    "            t = mel.shape[1]\n",
    "            num_paddings = hp.r - (t % hp.r) if t % hp.r != 0 else 0 # 0 for multiples\n",
    "\n",
    "            mel = np.pad(mel.T, [[0, num_paddings], [0, 0]], mode=\"minimum\")\n",
    "            mag = np.pad(mag.T, [[0, num_paddings], [0, 0]], mode=\"minimum\")\n",
    "\n",
    "            mel = mel.T\n",
    "            mag = mag.T\n",
    "\n",
    "            mel = mel.astype(np.float32)\n",
    "            mag = mag.astype(np.float32) # Default is float64, type crashes at the Attention Wrapper\n",
    "\n",
    "#             print('{:d}:{:d}'.format(first, last))\n",
    "\n",
    "            np.save(mag_path, mag)\n",
    "            np.save(mel_path, mel)\n",
    "\n",
    "#         print(fname)\n",
    "\n",
    "        ys.append(mel.T.reshape((-1, hp.n_mels*hp.r)))\n",
    "        mels.append(mel.T[hp.r - 1::hp.r, :]) # Reduce sample size by r\n",
    "        mags.append(mag.T)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    decoder_inputs = []\n",
    "\n",
    "    for i in range(len(mels)):\n",
    "        mel = mels[i]\n",
    "        decoder_input = tf.concat((tf.zeros_like(mel[:1, :]), mel[:-1, :]), 0)\n",
    "        decoder_inputs.append(decoder_input)\n",
    "        \n",
    "    ########################################################################################\n",
    "    \n",
    "    with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n",
    "        lookup_table = tf.get_variable('lookup_table', \n",
    "                                       dtype=tf.float32, \n",
    "                                       shape=[len(hp.vocab), hp.embed_size],\n",
    "                                       initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01))\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    embed_texts = []\n",
    "\n",
    "    with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n",
    "        for i in range(len(texts)):\n",
    "            text = texts[i]\n",
    "            embed_text = tf.nn.embedding_lookup(lookup_table, text)\n",
    "            embed_text = tf.expand_dims(embed_text, 0)\n",
    "            embed_texts.append(embed_text)\n",
    "\n",
    "    ########################################################################################\n",
    "    \n",
    "    prenet_results = []\n",
    "\n",
    "    for i in range(len(embed_texts)):\n",
    "        embed_text = embed_texts[i]\n",
    "        with tf.variable_scope(\"prenet\", reuse=tf.AUTO_REUSE):\n",
    "            outputs = tf.layers.dense(embed_text, units=hp.num_prenet_node_1, activation=tf.nn.relu, name=\"dense1\")\n",
    "            outputs = tf.layers.dropout(outputs, rate=hp.dropout_rate, name=\"dropout1\")\n",
    "            outputs = tf.layers.dense(outputs, units=hp.num_prenet_node_2, activation=tf.nn.relu, name=\"dense2\")\n",
    "            outputs = tf.layers.dropout(outputs, rate=hp.dropout_rate, name=\"dropout2\") \n",
    "\n",
    "        prenet_results.append(outputs)\n",
    "        \n",
    "    ########################################################################################\n",
    "    \n",
    "    conv_bank_results = []\n",
    "\n",
    "    for i in range(len(prenet_results)):\n",
    "        with tf.variable_scope(\"conv1d_banks\", reuse=tf.AUTO_REUSE):\n",
    "            prenet_result = prenet_results[i]\n",
    "\n",
    "            for k in range(1, hp.K + 1):\n",
    "                with tf.variable_scope(\"filter_num_{}\".format(k)):\n",
    "                    params = {\"inputs\":prenet_result, \"filters\":hp.num_k_filter, \"kernel_size\":k,\n",
    "                            \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                            \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "\n",
    "                    # Works when resue = True\n",
    "                    # For i loop, filter is reused.\n",
    "\n",
    "                    conv_outputs = tf.layers.conv1d(**params)\n",
    "                    if k == 1:\n",
    "                        conv_bank_outputs = conv_outputs\n",
    "                    else:\n",
    "                        conv_bank_outputs = tf.concat((conv_bank_outputs, conv_outputs), axis=2)\n",
    "\n",
    "        conv_bank_results.append(conv_bank_outputs)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    max_pooling_results = [];\n",
    "\n",
    "    for i in range(len(conv_bank_results)):\n",
    "        conv_bank_result = conv_bank_results[i]\n",
    "\n",
    "        max_pooled = tf.layers.max_pooling1d(conv_bank_result, pool_size=2, strides=1, padding=\"same\")\n",
    "\n",
    "        max_pooling_results.append(max_pooled)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    conv_proj_1_results = []\n",
    "\n",
    "    with tf.variable_scope(\"conv1d_1\"):\n",
    "        for i in range(len(max_pooling_results)):\n",
    "            max_pooling_result = max_pooling_results[i]\n",
    "\n",
    "            params = {\"inputs\":max_pooling_result, \"filters\":hp.num_conv1d_proj_filter, \"kernel_size\":hp.size_conv1d_proj_filter,\n",
    "                            \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                            \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "\n",
    "            conv_proj_1_result = tf.layers.conv1d(**params)\n",
    "\n",
    "            conv_proj_1_results.append(conv_proj_1_result)\n",
    "                \n",
    "    ########################################################################################\n",
    "    \n",
    "    batch_norm_1_results = []\n",
    "\n",
    "    with tf.variable_scope(\"conv1d_1\"):\n",
    "        for i in range(len(conv_proj_1_results)):\n",
    "            conv_proj_1_result = conv_proj_1_results[i]\n",
    "\n",
    "            bn_1_result = tf.contrib.layers.batch_norm(inputs=conv_proj_1_result,\n",
    "                                                   center=True,\n",
    "                                                   scale=True,\n",
    "                                                   updates_collections=None,\n",
    "                                                   is_training=True,\n",
    "                                                   scope=\"conv1d_1\", ## Am I Sure about this scope?\n",
    "                                                   fused=True,\n",
    "                                                   reuse=tf.AUTO_REUSE)\n",
    "            bn_1_act = tf.nn.relu(bn_1_result)\n",
    "\n",
    "            batch_norm_1_results.append(bn_1_act)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    conv_proj_2_results = []\n",
    "\n",
    "    with tf.variable_scope(\"conv1d_2\"):\n",
    "        for i in range(len(batch_norm_1_results)):\n",
    "            batch_norm_1_result = batch_norm_1_results[i]\n",
    "\n",
    "            params = {\"inputs\":batch_norm_1_result, \"filters\":hp.num_conv1d_proj_filter, \"kernel_size\":hp.size_conv1d_proj_filter,\n",
    "                            \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                            \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "\n",
    "            conv_proj_2_result = tf.layers.conv1d(**params)\n",
    "\n",
    "            conv_proj_2_results.append(conv_proj_2_result)\n",
    "\n",
    "    ########################################################################################\n",
    "    \n",
    "    batch_norm_2_results = []\n",
    "\n",
    "    with tf.variable_scope(\"conv1d_2\"):\n",
    "        for i in range(len(conv_proj_2_results)):\n",
    "            conv_proj_2_result = conv_proj_2_results[i]\n",
    "\n",
    "            bn_2_result = tf.contrib.layers.batch_norm(inputs=conv_proj_2_result,\n",
    "                                                   center=True,\n",
    "                                                   scale=True,\n",
    "                                                   updates_collections=None,\n",
    "                                                   is_training=True,\n",
    "                                                   scope=\"conv1d_2\", ## Am I Sure about this scope?\n",
    "                                                   fused=True,\n",
    "                                                   reuse=tf.AUTO_REUSE)\n",
    "            bn_2_act = tf.nn.relu(bn_2_result)\n",
    "\n",
    "            batch_norm_2_results.append(bn_2_act)\n",
    "    \n",
    "    ########################################################################################\n",
    "        \n",
    "    res_outputs = list(map(add, prenet_results, batch_norm_2_results))\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    for i in range(hp.num_highwaynet_blocks):\n",
    "        scope = \"highwaynet_{:d}\".format(i)\n",
    "        with tf.variable_scope(scope):\n",
    "\n",
    "            if i == 0:\n",
    "                highwaynet_inputs = res_outputs\n",
    "            else:\n",
    "                highwaynet_inputs = highwaynet_outputs\n",
    "\n",
    "            highwaynet_outputs = []\n",
    "\n",
    "            for j in range(len(highwaynet_inputs)):\n",
    "                highwaynet_input = highwaynet_inputs[j]\n",
    "\n",
    "                H = tf.layers.dense(highwaynet_input, units=hp.num_highwaynet_units, activation=tf.nn.relu, name=\"dense1\", reuse=tf.AUTO_REUSE)\n",
    "                T = tf.layers.dense(highwaynet_input, units=hp.num_highwaynet_units, activation=tf.nn.sigmoid,\n",
    "                                    bias_initializer=tf.constant_initializer(-1.0), name=\"dense2\", reuse=tf.AUTO_REUSE)\n",
    "                highwaynet_output = H*T + highwaynet_input*(1.-T)\n",
    "\n",
    "                highwaynet_outputs.append(highwaynet_output)\n",
    "            \n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    gru_results = []\n",
    "    with tf.variable_scope(\"gru\", reuse=tf.AUTO_REUSE):\n",
    "        for i in range(len(highwaynet_outputs)):\n",
    "    #     cell = tf.contrib.rnn.GRUCell(hp.num_gru_units)\n",
    "    #     cell_bw = tf.contrib.rnn.GRUCell(hp.num_gru_units)\n",
    "            cell = tf.contrib.rnn.GRUCell(128)\n",
    "            cell_bw = tf.contrib.rnn.GRUCell(128)\n",
    "\n",
    "            highwaynet_output = highwaynet_outputs[i]\n",
    "\n",
    "            output, _ = tf.nn.bidirectional_dynamic_rnn(cell, cell_bw, highwaynet_output, dtype=tf.float32)\n",
    "            output = tf.concat(output, 2)\n",
    "\n",
    "            gru_results.append(output)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    decoder_prenet_results = []\n",
    "\n",
    "    for i in range(len(decoder_inputs)):\n",
    "        decoder_input = decoder_inputs[i]\n",
    "        with tf.variable_scope(\"decoder_prenet\", reuse=tf.AUTO_REUSE):\n",
    "            outputs = tf.layers.dense(decoder_input, units=hp.num_prenet_node_1, activation=tf.nn.relu, name=\"dense1\")\n",
    "            outputs = tf.layers.dropout(outputs, rate=hp.dropout_rate, name=\"dropout1\")\n",
    "            outputs = tf.layers.dense(outputs, units=hp.num_prenet_node_2, activation=tf.nn.relu, name=\"dense2\")\n",
    "            outputs = tf.layers.dropout(outputs, rate=hp.dropout_rate, name=\"dropout2\") \n",
    "\n",
    "        decoder_prenet_results.append(outputs)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    decs, states = [], []\n",
    "\n",
    "    with tf.variable_scope(\"attention_decoder\", reuse=tf.AUTO_REUSE):\n",
    "\n",
    "        for i in range(len(gru_results)):\n",
    "            gru_result = gru_results[i]\n",
    "            decoder_prenet_result = tf.expand_dims(decoder_prenet_results[i], 0)\n",
    "\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(hp.num_attention_units, gru_result)\n",
    "            decoder_cell = tf.contrib.rnn.GRUCell(hp.num_gru_units)\n",
    "            cell_with_attention = tf.contrib.seq2seq.AttentionWrapper(decoder_cell,\n",
    "                                                                      attention_mechanism,\n",
    "                                                                      hp.num_attention_units,\n",
    "                                                                      alignment_history=True)\n",
    "            dec, state = tf.nn.dynamic_rnn(cell_with_attention, decoder_prenet_result, dtype=tf.float32)\n",
    "\n",
    "            decs.append(dec)\n",
    "            states.append(state)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    y_hats, alignments = [], []\n",
    "    \n",
    "    with tf.variable_scope(\"attention_decoder\", reuse=tf.AUTO_REUSE):\n",
    "\n",
    "        for i in range(len(states)):\n",
    "            state = states[i]\n",
    "            dec = decs[i]\n",
    "\n",
    "            alignment = tf.transpose(state.alignment_history.stack(),[1,2,0])\n",
    "\n",
    "            with tf.variable_scope(\"decoder_gru_1\", reuse=tf.AUTO_REUSE):\n",
    "                cell = tf.contrib.rnn.GRUCell(128)\n",
    "                output, _ = tf.nn.dynamic_rnn(cell, dec, dtype=tf.float32)\n",
    "                gru_output_1 = tf.concat(output, 2)\n",
    "\n",
    "            dec = dec + gru_output_1\n",
    "\n",
    "            with tf.variable_scope(\"decoder_gru_2\", reuse=tf.AUTO_REUSE):\n",
    "                cell = tf.contrib.rnn.GRUCell(128)\n",
    "                output, _ = tf.nn.dynamic_rnn(cell, dec, dtype=tf.float32)\n",
    "                gru_output_2 = tf.concat(output, 2)\n",
    "\n",
    "            dec = dec + gru_output_2\n",
    "\n",
    "            # Outputs => (N, T_y/r, n_mels*r)\n",
    "            y_hat = tf.layers.dense(dec, hp.n_mels*hp.r)\n",
    "\n",
    "            y_hats.append(y_hat)\n",
    "            alignments.append(alignment)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    dec_2_inputs = []\n",
    "\n",
    "    with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "        for i in range(len(y_hats)):\n",
    "            y_hat = y_hats[i]\n",
    "            dec_2_input = tf.reshape(y_hat, [1, -1, hp.n_mels])\n",
    "\n",
    "            dec_2_inputs.append(dec_2_input)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    dec_2_conv_bank_results = []\n",
    "\n",
    "    with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "        for i in range(len(dec_2_inputs)):\n",
    "            with tf.variable_scope(\"conv1d_banks\", reuse=tf.AUTO_REUSE):\n",
    "                dec_2_input = dec_2_inputs[i]\n",
    "\n",
    "                for k in range(1, hp.K + 1):\n",
    "                    with tf.variable_scope(\"filter_num_{}\".format(k)):\n",
    "                        params = {\"inputs\":dec_2_input, \"filters\":hp.num_k_filter, \"kernel_size\":k,\n",
    "                                \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                                \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "\n",
    "                        # Works when resue = True\n",
    "                        # For i loop, filter is reused.\n",
    "\n",
    "                        conv_outputs = tf.layers.conv1d(**params)\n",
    "                        if k == 1:\n",
    "                            conv_bank_outputs = conv_outputs\n",
    "                        else:\n",
    "                            conv_bank_outputs = tf.concat((conv_bank_outputs, conv_outputs), axis=2)\n",
    "\n",
    "            dec_2_conv_bank_results.append(conv_bank_outputs)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    dec_2_max_pooling_results = []\n",
    "\n",
    "    with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "        for i in range(len(dec_2_inputs)):\n",
    "            dec_2_conv_bank_result = dec_2_conv_bank_results[i]\n",
    "\n",
    "            max_pooled = tf.layers.max_pooling1d(dec_2_conv_bank_result, pool_size=2, strides=1, padding=\"same\")\n",
    "\n",
    "            dec_2_max_pooling_results.append(max_pooled)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    dec_2_conv_proj_1_results = []\n",
    "\n",
    "    with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "        with tf.variable_scope(\"conv1d_1\"):\n",
    "            for i in range(len(dec_2_max_pooling_results)):\n",
    "                dec_2_max_pooling_result = dec_2_max_pooling_results[i]\n",
    "\n",
    "                params = {\"inputs\":dec_2_max_pooling_result, \"filters\":hp.num_conv1d_proj_filter, \"kernel_size\":hp.size_conv1d_proj_filter,\n",
    "                                \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                                \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "\n",
    "                dec_2_conv_proj_1_result = tf.layers.conv1d(**params)\n",
    "\n",
    "                dec_2_conv_proj_1_results.append(dec_2_conv_proj_1_result)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    dec_2_batch_norm_1_results = []\n",
    "\n",
    "    with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "        with tf.variable_scope(\"conv1d_1\"):\n",
    "            for i in range(len(dec_2_conv_proj_1_results)):\n",
    "                dec_2_conv_proj_1_result = dec_2_conv_proj_1_results[i]\n",
    "\n",
    "                bn_1_result = tf.contrib.layers.batch_norm(inputs=dec_2_conv_proj_1_result,\n",
    "                                                       center=True,\n",
    "                                                       scale=True,\n",
    "                                                       updates_collections=None,\n",
    "                                                       is_training=True,\n",
    "                                                       scope=\"conv1d_1\",\n",
    "                                                       fused=True,\n",
    "                                                       reuse=tf.AUTO_REUSE)\n",
    "                bn_1_act = tf.nn.relu(bn_1_result)\n",
    "\n",
    "                dec_2_batch_norm_1_results.append(bn_1_act)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    dec_2_conv_proj_2_results = []\n",
    "\n",
    "    with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "        with tf.variable_scope(\"conv1d_2\"):\n",
    "            for i in range(len(dec_2_batch_norm_1_results)):\n",
    "                dec_2_batch_norm_1_result = dec_2_batch_norm_1_results[i]\n",
    "\n",
    "                params = {\"inputs\":dec_2_batch_norm_1_result, \"filters\":hp.num_conv1d_proj_filter, \"kernel_size\":hp.size_conv1d_proj_filter,\n",
    "                                \"dilation_rate\":1, \"padding\":\"SAME\", \"activation\":None, \n",
    "                                \"use_bias\":False, \"reuse\":tf.AUTO_REUSE}\n",
    "\n",
    "                dec_2_conv_proj_2_result = tf.layers.conv1d(**params)\n",
    "\n",
    "                dec_2_conv_proj_2_results.append(dec_2_conv_proj_2_result)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    dec_2_batch_norm_2_results = []\n",
    "\n",
    "    with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "        with tf.variable_scope(\"conv1d_2\"):\n",
    "            for i in range(len(dec_2_conv_proj_2_results)):\n",
    "                dec_2_conv_proj_2_result = dec_2_conv_proj_2_results[i]\n",
    "\n",
    "                bn_2_result = tf.contrib.layers.batch_norm(inputs=dec_2_conv_proj_2_result,\n",
    "                                                       center=True,\n",
    "                                                       scale=True,\n",
    "                                                       updates_collections=None,\n",
    "                                                       is_training=True,\n",
    "                                                       scope=\"conv1d_2\",\n",
    "                                                       fused=True,\n",
    "                                                       reuse=tf.AUTO_REUSE)\n",
    "                bn_2_act = tf.nn.relu(bn_2_result)\n",
    "                dec_2_batch_norm_2_results.append(bn_2_act)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    dec_2_sync_results = []\n",
    "\n",
    "    with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "            for i in range(len(dec_2_batch_norm_2_results)):\n",
    "                dec_2_batch_norm_2_result = dec_2_batch_norm_2_results[i]\n",
    "\n",
    "                dec_2_sync_result = tf.layers.dense(dec_2_batch_norm_2_result, 128)\n",
    "                dec_2_sync_results.append(dec_2_sync_result)\n",
    "\n",
    "    ########################################################################################\n",
    "    \n",
    "    with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "        for i in range(hp.num_highwaynet_blocks):\n",
    "            scope = \"highwaynet_{:d}\".format(i)\n",
    "            with tf.variable_scope(scope):\n",
    "\n",
    "                if i == 0:\n",
    "                    highwaynet_inputs = dec_2_sync_results\n",
    "                else:\n",
    "                    highwaynet_inputs = highwaynet_outputs\n",
    "\n",
    "                highwaynet_outputs = []\n",
    "\n",
    "                for j in range(len(highwaynet_inputs)):\n",
    "                    highwaynet_input = highwaynet_inputs[j]\n",
    "\n",
    "                    H = tf.layers.dense(highwaynet_input, units=hp.num_highwaynet_units, activation=tf.nn.relu, name=\"dense1\", reuse=tf.AUTO_REUSE)\n",
    "                    T = tf.layers.dense(highwaynet_input, units=hp.num_highwaynet_units, activation=tf.nn.sigmoid,\n",
    "                                        bias_initializer=tf.constant_initializer(-1.0), name=\"dense2\", reuse=tf.AUTO_REUSE)\n",
    "                    highwaynet_output = H*T + highwaynet_input*(1.-T)\n",
    "\n",
    "                    highwaynet_outputs.append(highwaynet_output)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    dec_2_gru_results = []\n",
    "    with tf.variable_scope(\"gru\", reuse=tf.AUTO_REUSE):\n",
    "        for i in range(len(highwaynet_outputs)):\n",
    "            cell = tf.contrib.rnn.GRUCell(128)\n",
    "            cell_bw = tf.contrib.rnn.GRUCell(128)\n",
    "\n",
    "            highwaynet_output = highwaynet_outputs[i]\n",
    "\n",
    "            output, _ = tf.nn.bidirectional_dynamic_rnn(cell, cell_bw, highwaynet_output, dtype=tf.float32)\n",
    "            output = tf.concat(output, 2)\n",
    "\n",
    "            dec_2_gru_results.append(output)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    z_hats = []\n",
    "\n",
    "    with tf.variable_scope(\"decoder2\", reuse=tf.AUTO_REUSE):\n",
    "        with tf.variable_scope(\"final\", reuse=tf.AUTO_REUSE):\n",
    "            for i in range(len(dec_2_gru_results)):\n",
    "                dec_2_gru_result = dec_2_gru_results[i]\n",
    "\n",
    "                dec_2_result = tf.layers.dense(dec_2_gru_result, 1 + hp.nsc_sec*fs//2)\n",
    "                z_hats.append(dec_2_result)\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    delta_y = lambda y_hat, y: y_hat - y, y_hats, ys\n",
    "    \n",
    "    delta_ys = []\n",
    "\n",
    "    for i in range(len(y_hats)):\n",
    "        delta_y = abs(y_hats[i][0, :, :] - ys[i])\n",
    "        delta_ys.append(tf.reduce_mean(delta_y))\n",
    "\n",
    "    delta_zs = []\n",
    "\n",
    "    for i in range(len(z_hats)):\n",
    "        delta_z = abs(z_hats[i][0, :, :] - mags[i])\n",
    "        delta_zs.append(tf.reduce_mean(delta_z))\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    loss1 = tf.reduce_mean(delta_ys)\n",
    "    loss2 = tf.reduce_mean(delta_zs)\n",
    "    loss = loss1 + loss2\n",
    "    \n",
    "    ########################################################################################\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    lr = learning_rate_decay(hp.lr, global_step=global_step)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr)\n",
    "\n",
    "    ## gradient clipping\n",
    "    gvs = optimizer.compute_gradients(loss)\n",
    "    clipped = []\n",
    "\n",
    "    for grad, var in gvs:\n",
    "        grad = tf.clip_by_norm(grad, 5.)\n",
    "        clipped.append((grad, var))\n",
    "\n",
    "    with tf.variable_scope('optimizer', reuse=tf.AUTO_REUSE):\n",
    "        train_op = optimizer.apply_gradients(clipped, global_step=global_step)\n",
    "    \n",
    "    ########################################################################################\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run([train_op, global_step])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
